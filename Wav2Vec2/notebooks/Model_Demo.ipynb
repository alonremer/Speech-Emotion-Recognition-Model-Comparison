{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN/XHeGnbN+p8Ho/S96zTFA"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import os\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from IPython.display import Audio\n","from sklearn.preprocessing import OneHotEncoder\n","\n","import torch\n","import torchaudio\n","import torchvision.transforms as transforms\n","from torch.utils.data import DataLoader, Dataset\n","from torchaudio.transforms import MelSpectrogram, AmplitudeToDB\n","from torchvision.models import resnet18\n","import torch.nn as nn\n","import torch.optim as optim\n","from google.colab import drive\n","\n","from google.colab import drive\n","import os\n","# Mount Google Drive\n","drive.mount('/content/drive')\n","\n","# Ensure CUDA is available\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BmimSz0VU0VW","executionInfo":{"status":"ok","timestamp":1723981906156,"user_tz":-180,"elapsed":47020,"user":{"displayName":"Alon Remer","userId":"06241924086891510447"}},"outputId":"fec4dd6b-d5ce-4573-84ed-54d6e179eaa5"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":900,"output_embedded_package_id":"1hZNb5hCZMiZbFQsHWd0elI9cxRqD7dML"},"id":"Am6lQrHBUorB","executionInfo":{"status":"ok","timestamp":1723983288559,"user_tz":-180,"elapsed":6506,"user":{"displayName":"Alon Remer","userId":"06241924086891510447"}},"outputId":"c317e377-e09b-4204-da8d-c770122774f8"},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["from transformers import Wav2Vec2FeatureExtractor, Wav2Vec2ForSequenceClassification\n","import torch\n","import os\n","import torchaudio\n","import torch.nn.functional as F\n","from IPython.display import display, Audio, HTML\n","\n","# Initialize the feature extractor and model\n","feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(\"facebook/wav2vec2-base\")\n","model_save_path = os.path.join(\"/content/drive/My Drive\", \"Final_Model_Aug\")\n","model = Wav2Vec2ForSequenceClassification.from_pretrained(model_save_path)\n","model.eval()\n","\n","def preprocess_audio(audio_path):\n","    waveform, sample_rate = torchaudio.load(audio_path)\n","\n","    # Ensure the waveform is mono (single channel)\n","    if waveform.size(0) > 1:\n","        waveform = waveform.mean(dim=0, keepdim=True)\n","\n","    # Resample if needed\n","    if sample_rate != feature_extractor.sampling_rate:\n","        resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=feature_extractor.sampling_rate)\n","        waveform = resampler(waveform)\n","\n","    # Extract features with padding and truncation\n","    inputs = feature_extractor(\n","        waveform.squeeze().numpy(),\n","        sampling_rate=feature_extractor.sampling_rate,\n","        return_tensors=\"pt\",\n","        padding=True,\n","        truncation=True,\n","        max_length=40000  # Set a fixed max_length value\n","    )\n","\n","    # Return input values with the correct shape\n","    return inputs.input_values.squeeze(0)  # Ensure it's [sequence_length] or [batch_size, sequence_length]\n","\n","# Paths to your WAV files\n","wav_file_paths = [\n","    os.path.join(\"/content/drive/My Drive/Final_Model_Aug\", \"Hadar-angry.wav\"),\n","    os.path.join(\"/content/drive/My Drive/Final_Model_Aug\", \"Hadar-happy (1).wav\"),\n","    os.path.join(\"/content/drive/My Drive/Final_Model_Aug\", \"Hadar-happy (2).wav\"),\n","    os.path.join(\"/content/drive/My Drive/Final_Model_Aug\", \"Hadar-NEU.wav\")\n","]\n","\n","# Device configuration\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)\n","\n","# Label names\n","label_names = ['neutral', 'happy', 'fear', 'angry', 'disgust', 'sad']\n","\n","# Processing and displaying results\n","for wav_file_path in wav_file_paths:\n","    input_values = preprocess_audio(wav_file_path)\n","\n","    # Ensure input values are 2D tensor (batch_size, sequence_length)\n","    input_values = input_values.unsqueeze(0).to(device)  # Add batch dimension\n","\n","    with torch.no_grad():\n","        outputs = model(input_values)\n","        logits = outputs.logits\n","        probabilities = F.softmax(logits.squeeze(), dim=0)\n","\n","    predicted_id = torch.argmax(logits, dim=-1).item()\n","    predicted_label = label_names[predicted_id]\n","\n","    # Display audio and probabilities\n","    display(HTML(f\"<h3>Predicted emotion for '{os.path.basename(wav_file_path)}': {predicted_label}</h3>\"))\n","    display(Audio(wav_file_path))\n","\n","    # Display the probabilities in a more readable format\n","    prob_percentages = (probabilities * 100).tolist()\n","    for label, prob in zip(label_names, prob_percentages):\n","        print(f\"{label}: {prob:.2f}%\")\n","\n","    print(\"-\" * 50)  # Separator for readability\n"]}]}